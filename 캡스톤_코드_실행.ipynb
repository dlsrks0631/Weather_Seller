{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**데이터 전처리 및 기계학습을 위한 코드(이수연님 제작)**"
      ],
      "metadata": {
        "id": "B8rYmwP_L7xY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY8Dk3qxLry2",
        "outputId": "312ed5fe-3b94-4568-fa3f-b237ed8a083c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-02 04:27:14.431887\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 74139427.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Epoch 1, Loss: 1.5907240845763684\n",
            "Epoch 2, Loss: 1.2895592569571734\n",
            "Finished Training\n",
            "2023-11-02 04:33:33.010544\n",
            "     Unnamed: 0    prtymd  apr_tizn_c ana_mgpo_nm ana_ccd_nm  ma_fem_dc  \\\n",
            "41           42  20220101          10        전라북도     전주시덕진구          2   \n",
            "42           43  20220101          10        전라북도     전주시완산구          1   \n",
            "43           44  20220101          10        전라북도     전주시완산구          1   \n",
            "131         132  20220101           4        전라북도     전주시덕진구          1   \n",
            "132         133  20220101           4        전라북도     전주시덕진구          1   \n",
            "\n",
            "     age_grp  wedd_bit  och_exs_bit  inf_area tco_btc     sl_am     sl_ct  \n",
            "41         5         0            0         1  기타일반음식  0.020888  0.234623  \n",
            "42         3         0            0         0     유통업  0.141868  0.527716  \n",
            "43         6         0            0         0  기타일반음식 -0.323440 -0.373275  \n",
            "131        3         9            9         1      연료 -0.406944 -0.590381  \n",
            "132        4         0            0         1  카페/디저트 -0.071168  1.624103  \n"
          ]
        }
      ],
      "source": [
        "# vscode로 실행(전라북도 한정)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def main():\n",
        "    print(datetime.now())\n",
        "\n",
        "    # 데이터 불러오기\n",
        "    filtered_data_list = []\n",
        "    chunk_iter = pd.read_csv(r\"/content/drive/MyDrive/2023-10-10-롯데카드_소비_데이터.csv\", encoding='CP949', chunksize=1000000)\n",
        "    # 데이터는 먼저 저의 Google Drive 파일들을 마운트한 뒤, 그곳에 기업 데이터를 저장한 뒤, 파일을 우클릭하여 경로를 복사하시면 됩니다.\n",
        "\n",
        "    for chunk in chunk_iter:\n",
        "        filtered_chunk = chunk[chunk['ana_mgpo_nm'] == '전라북도']\n",
        "        filtered_data_list.append(filtered_chunk)\n",
        "\n",
        "    filtered_data = pd.concat(filtered_data_list, axis=0)\n",
        "\n",
        "    # 결측치 처리\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    filtered_data[\"sl_am\"] = imputer.fit_transform(filtered_data[[\"sl_am\"]])\n",
        "\n",
        "    # 이상치 처리\n",
        "    z_scores = np.abs((filtered_data[\"sl_am\"] - filtered_data[\"sl_am\"].mean()) / filtered_data[\"sl_am\"].std())\n",
        "    filtered_data = filtered_data[(z_scores < 3)]\n",
        "\n",
        "    # 정규화\n",
        "    scaler = StandardScaler()\n",
        "    filtered_data[[\"sl_am\", \"sl_ct\"]] = scaler.fit_transform(filtered_data[[\"sl_am\", \"sl_ct\"]])\n",
        "\n",
        "    # 데이터 분석 및 기술 개발 부분은 문제에 따라 추가적인 코드를 작성해야 합니다.\n",
        "\n",
        "    # 전처리된 데이터를 파일로 저장 (옵션)\n",
        "    filtered_data.to_csv(\"preprocessed_filtered_data.csv\", index=False)\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            self.fc1 = nn.Linear(6 * 14 * 14, 120)\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool(torch.relu(self.conv1(x)))\n",
        "            x = x.view(-1, 6 * 14 * 14)\n",
        "            x = torch.relu(self.fc1(x))\n",
        "            x = torch.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "    net = Net()\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(2):  # 데이터셋을 두 번 반복합니다.\n",
        "        running_loss = 0.0\n",
        "        for i, data_batch in enumerate(trainloader, 0):\n",
        "            inputs, labels = data_batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(datetime.now())\n",
        "    print(filtered_data.head())\n",
        "    return filtered_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}